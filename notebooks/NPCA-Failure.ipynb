{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "from joblib import parallel_backend\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import pickle\n",
    "\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import ingest\n",
    "from scipy import sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Reading local cache file Outpatient.pkl\n"
     ]
    }
   ],
   "source": [
    "outpatient = ingest.get_cache_data(\"Outpatient\", \"Outpatient.pkl\")\n",
    "outpatient.dropna(subset=['CLM_PMT_AMT'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIRTH_DATE\n",
      "(790790, 1)\n",
      "SEX\n",
      "(790790, 1)\n",
      "State\n",
      "(790790, 52)\n",
      "County\n",
      "(790790, 306)\n",
      "clm_dates\n",
      "(790790, 1)\n",
      "provider\n",
      "(790790, 284)\n",
      "provider_count\n",
      "(790790, 1)\n",
      "DGNS_CD\n",
      "(790790, 1001)\n",
      "DGNS_CD_count\n",
      "(790790, 1)\n",
      "PRDCR_CD\n",
      "(790790, 188)\n",
      "PRDCR_CD_count\n",
      "(790790, 1)\n",
      "HCPCS_CD\n",
      "(790790, 730)\n",
      "HCPCS_CD_count\n",
      "(790790, 1)\n"
     ]
    }
   ],
   "source": [
    "with open('feature_vectors_dictionary-truncated.txt', 'rb') as handle:\n",
    "    feature_dictionary = pickle.load(handle)\n",
    "\n",
    "for met_set in feature_dictionary.keys():\n",
    "    print(met_set)\n",
    "    feature_dictionary[met_set] = normalize(sparse.csr_matrix(feature_dictionary[met_set]))\n",
    "    print(np.shape(feature_dictionary[met_set]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(X, y, model, grid, feature_names):\n",
    "\n",
    "       ### Runs the model and returns MSE and R2 values and corresponding data as a dataframe\n",
    "\n",
    "       ### Takes in four arugments\n",
    "       ### X is the features to that model will be trained on as a sparse matrix\n",
    "       ### y is the scalar that is to be predicted as a list\n",
    "       ### model is the regression model as a model object\n",
    "       ### grid is a dictionary of model hyperparameters\n",
    "\n",
    "       ### returns a dataframe of results from the grid search\n",
    "\n",
    "       scoring = {'mean_squared_error': make_scorer(mean_squared_error), 'r2_score': make_scorer(r2_score)}\n",
    "       with parallel_backend('threading', n_jobs=2):\n",
    "              grid_model = GridSearchCV(model, grid, scoring=scoring, refit='r2_score', return_train_score=True, error_score=\"raise\").fit(X.todense(), y)\n",
    "       \n",
    "\n",
    "\n",
    "       preds = grid_model.predict(X.todense())\n",
    "       print(preds)\n",
    "       df = outpatient.copy()\n",
    "\n",
    "       df['predictions'] =  preds\n",
    "       \n",
    "       return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_feature_sets(model, feature_dict, grid, feature_sets):\n",
    "\n",
    "    ### Creates and runs the specified model with each comboniation of features sets from 1 to k length features and stores the results of the models into a dictionary\n",
    "\n",
    "    ### Takes in four arguments\n",
    "    ### model is the regression model as a model object\n",
    "    ### feature_dict is the dictionary of feature sets\n",
    "    ### grid is a dictionary of model hyperparameters\n",
    "    ### feature_sets dicates how what feature sets will be returned, a list of list of strings corresponding to the keys in the dictionary\n",
    "    ### (continued) all would return all combinations for 1 through k, where  k is the number of keys in feature_dict\n",
    "\n",
    "    ### returns a dictionary of results\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for feature_list in feature_sets:\n",
    "        if feature_list == 'all':\n",
    "\n",
    "            for n_met in tqdm(range(len(feature_dict))):\n",
    "                for met_set in list(itertools.combinations(feature_dict.keys(), n_met+1)):\n",
    "                    for n_met_set in range(len(met_set)):\n",
    "                        if n_met_set == 0:\n",
    "                            X = feature_dict[met_set[n_met_set]]\n",
    "                        else:\n",
    "                            X  = sparse.hstack((X,feature_dict[met_set[n_met_set]]), format='csr')\n",
    "                    answer = run_model(X, outpatient['CLM_PMT_AMT'].values.tolist(), model, grid, met_set)\n",
    "                    results[met_set] = answer\n",
    "\n",
    "        else:\n",
    "            X = None\n",
    "            Name = ()\n",
    "            for i in feature_list:\n",
    "                if X == None:\n",
    "                    X = feature_dict[i]\n",
    "                    Name += (i,)\n",
    "                else:\n",
    "                    X = sparse.hstack((X, feature_dict[i]), format='csr')\n",
    "                    Name += (i,)\n",
    "            answer = run_model(X, outpatient['CLM_PMT_AMT'].values.tolist(), model, grid, feature_list)\n",
    "            results[Name] = answer\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results(results_dict, filename):\n",
    "\n",
    "    ### Takes in two arguements\n",
    "    ### results_dict is the dictionary of feature sets\n",
    "    ### file name is the wanted file name as a string\n",
    "\n",
    "    ### Returns None, but writes a file to the current directory\n",
    "\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for r_key in results_dict.keys():\n",
    "        temp_df = results_dict[r_key]\n",
    "        temp_df['features'] = str(r_key)\n",
    "        df = pd.concat((df, temp_df), ignore_index=True)\n",
    "\n",
    "    df.to_csv(filename)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Histogram Gradient Boosted NonPCA Dataset\n",
    "### Generates Dataset with Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 73.44122528 208.89277945 257.55470048 ... 183.601469   129.96102677\n",
      " 296.50111943]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "grid_dict = {'learning_rate': [0.25],\n",
    "    'max_iter': [200],\n",
    "    #wanted to go a little higher on leaf nodes since there seemed to be more to gain\n",
    "    'max_leaf_nodes': [10],\n",
    "    'max_depth': [18]\n",
    "    #results were inconclusive for minimum samples, going to leave as default\n",
    "    }\n",
    "\n",
    "\n",
    "GBRModel = HistGradientBoostingRegressor(random_state=42)\n",
    "write_results(iterate_feature_sets(GBRModel, feature_dictionary, grid_dict, [\n",
    "    # picked because of speed and results\n",
    "    ['clm_dates', 'DGNS_CD', 'HCPCS_CD', 'BIRTH_DATE', 'SEX', 'State', 'County']]), 'NPCA_FinalData_GBRModel.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Stochastic Gradient Descent NonPCA Supervised Run\n",
    "### See how the model performs on the dataset without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[153.2365103   99.1009535  234.30567492 ... 195.1020327  142.01869419\n",
      " 355.95122977]\n"
     ]
    }
   ],
   "source": [
    "grid_dict = {'loss': ['squared_epsilon_insensitive'],\n",
    "    'penalty': ['l1'],\n",
    "    'alpha': [.0001],\n",
    "    'max_iter': [1000]}\n",
    "\n",
    "SGDModel = SGDRegressor(random_state=42)\n",
    "write_results(iterate_feature_sets(SGDModel, feature_dictionary, grid_dict, [\n",
    "['clm_dates', 'DGNS_CD', 'HCPCS_CD', 'BIRTH_DATE', 'SEX', 'State', 'County']]), 'NPCA_FinalData_SGDModel.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Elastic Net NonPCA Supervised Run\n",
    "### See how the model performs on the dataset without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[138.27836695  97.18758939 230.0426669  ... 196.25697381 140.32453803\n",
      " 356.97479306]\n"
     ]
    }
   ],
   "source": [
    "grid_dict = {'alpha': [.0001],\n",
    "    'l1_ratio': [.9]}\n",
    "\n",
    "ENModel = ElasticNet(random_state=42)\n",
    "write_results(iterate_feature_sets(ENModel, feature_dictionary, grid_dict, [\n",
    "['clm_dates', 'DGNS_CD', 'HCPCS_CD', 'BIRTH_DATE', 'SEX', 'State', 'County']]), 'NPCA_FinalData_ENModel.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final XGBoost NonPCA Supervised Run\n",
    "### See how the model performs on the dataset without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:47:14] WARNING: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-08de971ced8a8cdc6-1/xgboost/xgboost-ci-windows/src/learner.cc:767: \n",
      "Parameters: { \"number_of_estimators\" } are not used.\n",
      "\n",
      "[ 77.02148 330.28763 240.97156 ... 241.60034 118.74428 236.9609 ]\n"
     ]
    }
   ],
   "source": [
    "grid_dict = {'max_depth': [10],\n",
    "    'number_of_estimators': [9],\n",
    "    'tree_method': ['hist']}\n",
    "\n",
    "XGBModel = xgb.XGBRFRegressor(random_state=42)\n",
    "write_results(iterate_feature_sets(XGBModel, feature_dictionary, grid_dict, [\n",
    "['clm_dates', 'DGNS_CD', 'HCPCS_CD', 'BIRTH_DATE', 'SEX', 'State', 'County']]), 'NPCA_FinalData_XGBModel.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
