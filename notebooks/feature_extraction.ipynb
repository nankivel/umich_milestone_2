{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "import ingest\n",
    "\n",
    "from scipy import sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Reading local cache file Outpatient.pkl\n"
     ]
    }
   ],
   "source": [
    "outpatient = ingest.get_cache_data(\"Outpatient\", \"Outpatient.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpatient.dropna(subset=['CLM_PMT_AMT'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_sets_dict = {'BIRTH_DATE': ['BENE_BIRTH_DT'],\n",
    "    'SEX': ['BENE_SEX_IDENT_CD'],\n",
    "    'State': ['SP_STATE_CODE'],\n",
    "    'County': ['BENE_COUNTY_CD'], \n",
    "    'clm_dates': ['CLM_FROM_DT', 'CLM_THRU_DT'],\n",
    "    'provider': ['PRVDR_NUM'],\n",
    "    'DGNS_CD': ['ICD9_DGNS_CD_1', 'ICD9_DGNS_CD_2', 'ICD9_DGNS_CD_3',\n",
    "    'ICD9_DGNS_CD_4', 'ICD9_DGNS_CD_5', 'ICD9_DGNS_CD_6', 'ICD9_DGNS_CD_7',\n",
    "    'ICD9_DGNS_CD_8', 'ICD9_DGNS_CD_9','ICD9_DGNS_CD_10'],\n",
    "    'PRDCR_CD': ['ICD9_PRCDR_CD_1', 'ICD9_PRCDR_CD_2', 'ICD9_PRCDR_CD_3',\n",
    "    'ICD9_PRCDR_CD_4', 'ICD9_PRCDR_CD_5', 'ICD9_PRCDR_CD_6'],\n",
    "    'HCPCS_CD': ['HCPCS_CD_1', 'HCPCS_CD_2', 'HCPCS_CD_3', 'HCPCS_CD_4',\n",
    "    'HCPCS_CD_5', 'HCPCS_CD_6', 'HCPCS_CD_7', 'HCPCS_CD_8', 'HCPCS_CD_9',\n",
    "    'HCPCS_CD_10', 'HCPCS_CD_11', 'HCPCS_CD_12', 'HCPCS_CD_13', 'HCPCS_CD_14',\n",
    "    'HCPCS_CD_15', 'HCPCS_CD_16', 'HCPCS_CD_17', 'HCPCS_CD_18', 'HCPCS_CD_19',\n",
    "    'HCPCS_CD_20', 'HCPCS_CD_21', 'HCPCS_CD_22', 'HCPCS_CD_23', 'HCPCS_CD_24',\n",
    "    'HCPCS_CD_25', 'HCPCS_CD_26', 'HCPCS_CD_27', 'HCPCS_CD_28', 'HCPCS_CD_29',\n",
    "    'HCPCS_CD_30', 'HCPCS_CD_31', 'HCPCS_CD_32', 'HCPCS_CD_33', 'HCPCS_CD_34',\n",
    "    'HCPCS_CD_35', 'HCPCS_CD_36', 'HCPCS_CD_37', 'HCPCS_CD_38', 'HCPCS_CD_39',\n",
    "    'HCPCS_CD_40', 'HCPCS_CD_41', 'HCPCS_CD_42', 'HCPCS_CD_43', 'HCPCS_CD_44',\n",
    "    'HCPCS_CD_45']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isnan(x):\n",
    "    return x != x\n",
    "\n",
    "def TFIDF_Matrix3(df, columns):\n",
    "    #Takes in a dataframe, and a list of columns that comprise a \"sentence\" using the first three characters of each word\n",
    "    #Returns a TFIDF matrix\n",
    "\n",
    "    corpus = []\n",
    "\n",
    "    if len(columns) > 1:\n",
    "        values = df[columns].values.tolist()\n",
    "\n",
    "        for sentence in values:\n",
    "            s = ''\n",
    "            for word in sentence:\n",
    "                if len(s) == 0:\n",
    "                    if isnan(word):\n",
    "                        s = 'None, '\n",
    "                    else:\n",
    "                        s = str(word)[:3]+' ' #Add [:3] after str(word) to truncate to the first 3 characters\n",
    "                else:\n",
    "                    if isnan(word):\n",
    "                        s += 'None, '\n",
    "                    else:\n",
    "                        s += str(word)[:3]+' ' #Add [:3] after str(word) to truncate to the first 3 characters\n",
    "            corpus.append(s)\n",
    "\n",
    "    else:\n",
    "        values = df[columns].values.tolist()\n",
    "        for word in values:\n",
    "            if isnan(word[0]):\n",
    "                corpus.append('None')\n",
    "            else:\n",
    "                corpus.append(word[0][:3]) #Add [:3] after word[0] to truncate to the first 3 characters\n",
    "    \n",
    "    a = CountVectorizer(stop_words=['None']).fit_transform(corpus)\n",
    "\n",
    "    return  np.nan_to_num(a), np.nan_to_num(np.sum(a,axis=1))\n",
    "\n",
    "def TFIDF_Matrix(df, columns):\n",
    "    #Takes in a dataframe, and a list of columns that comprise a \"sentence\" using the complete \"word\"\n",
    "    #Returns a TFIDF matrix\n",
    "\n",
    "    corpus = []\n",
    "\n",
    "    if len(columns) > 1:\n",
    "        values = df[columns].values.tolist()\n",
    "\n",
    "        for sentence in values:\n",
    "            s = ''\n",
    "            for word in sentence:\n",
    "                if len(s) == 0:\n",
    "                    if isnan(word):\n",
    "                        s = 'None, '\n",
    "                    else:\n",
    "                        s = str(word)+' '\n",
    "                else:\n",
    "                    if isnan(word):\n",
    "                        s += 'None, '\n",
    "                    else:\n",
    "                        s += str(word)+' '\n",
    "            corpus.append(s)\n",
    "\n",
    "    else:\n",
    "        values = df[columns].values.tolist()\n",
    "        for word in values:\n",
    "            if isnan(word[0]):\n",
    "                corpus.append('None')\n",
    "            else:\n",
    "                corpus.append(word[0])\n",
    "\n",
    "    a = CountVectorizer(stop_words=['None']).fit_transform(corpus)\n",
    "\n",
    "    return  np.nan_to_num(a)\n",
    "\n",
    "def Date_Year(df, columns):\n",
    "\n",
    "    df['year'] = pd.DatetimeIndex(df[columns[0]]).year\n",
    "\n",
    "    return np.nan_to_num(np.array(df['year'].values.tolist()).reshape(len(df),1))\n",
    "\n",
    "def Date_Diff(df, columns):\n",
    "    #Takes in a dataframe, and a list of columns that are a start date and end date\n",
    "    #Returns the difference between the two dates\n",
    "\n",
    "    values = df[columns]\n",
    "\n",
    "    values['date_diff'] = (df[columns[1]] - df[columns[0]])  / np.timedelta64(1, 'D')\n",
    "\n",
    "    return  np.nan_to_num(np.array(values['date_diff'].values.tolist()).reshape(len(values),1) / np.array(values['date_diff'].values.tolist()).reshape(len(values),1).max(axis=0))\n",
    "    \n",
    "def Passthrough(df, columns):\n",
    "    #Takes in a dataframe, and a list of columns that need to be turned into a list\n",
    "    #Returns list of values from columns\n",
    "    \n",
    "    df[columns[0]] = pd.to_numeric(df[columns[0]], errors='coerce').astype('Int64')\n",
    "\n",
    "    return np.nan_to_num(np.array(df[columns].values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_sets_matrix_dict = {}\n",
    "for key, value in column_sets_dict.items():\n",
    "    if key in ['provider', 'NPI', 'admit_code', 'claim_discharge_code', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD']:\n",
    "        column_sets_matrix_dict[key], column_sets_matrix_dict[key+'_count'] = TFIDF_Matrix3(outpatient, value)\n",
    "    elif key in ['State', 'County']:\n",
    "        column_sets_matrix_dict[key] = TFIDF_Matrix(outpatient, value)\n",
    "    elif key in ['clm_dates', 'admit_dates']:\n",
    "        column_sets_matrix_dict[key] = Date_Diff(outpatient, value)\n",
    "    elif key in ['BIRTH_DATE']:\n",
    "        column_sets_matrix_dict[key] = Date_Year(outpatient, value)\n",
    "    else:\n",
    "        column_sets_matrix_dict[key] = Passthrough(outpatient, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"feature_vectors_dictionary-truncated.txt\", \"wb\") as f:\n",
    "    pickle.dump(column_sets_matrix_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (1, 0)\t1.0\n",
      "  (2, 0)\t1.0\n",
      "  (3, 0)\t1.0\n",
      "  (4, 0)\t1.0\n",
      "  (5, 0)\t1.0\n",
      "  (6, 0)\t1.0\n",
      "  (7, 0)\t1.0\n",
      "  (8, 0)\t1.0\n",
      "  (9, 0)\t1.0\n",
      "  (10, 0)\t1.0\n",
      "  (11, 0)\t1.0\n",
      "  (12, 0)\t1.0\n",
      "  (13, 0)\t1.0\n",
      "  (14, 0)\t1.0\n",
      "  (15, 0)\t1.0\n",
      "  (16, 0)\t1.0\n",
      "  (17, 0)\t1.0\n",
      "  (18, 0)\t1.0\n",
      "  (19, 0)\t1.0\n",
      "  (20, 0)\t1.0\n",
      "  (21, 0)\t1.0\n",
      "  (22, 0)\t1.0\n",
      "  (23, 0)\t1.0\n",
      "  (24, 0)\t1.0\n",
      "  :\t:\n",
      "  (790765, 0)\t1.0\n",
      "  (790766, 0)\t1.0\n",
      "  (790767, 0)\t1.0\n",
      "  (790768, 0)\t1.0\n",
      "  (790769, 0)\t1.0\n",
      "  (790770, 0)\t1.0\n",
      "  (790771, 0)\t1.0\n",
      "  (790772, 0)\t1.0\n",
      "  (790773, 0)\t1.0\n",
      "  (790774, 0)\t1.0\n",
      "  (790775, 0)\t1.0\n",
      "  (790776, 0)\t1.0\n",
      "  (790777, 0)\t1.0\n",
      "  (790778, 0)\t1.0\n",
      "  (790779, 0)\t1.0\n",
      "  (790780, 0)\t1.0\n",
      "  (790781, 0)\t1.0\n",
      "  (790782, 0)\t1.0\n",
      "  (790783, 0)\t1.0\n",
      "  (790784, 0)\t1.0\n",
      "  (790785, 0)\t1.0\n",
      "  (790786, 0)\t1.0\n",
      "  (790787, 0)\t1.0\n",
      "  (790788, 0)\t1.0\n",
      "  (790789, 0)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(column_sets_matrix_dict['HCPCS_CD_count'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
