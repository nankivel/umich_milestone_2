{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "from joblib import parallel_backend\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import pickle\n",
    "\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import ingest\n",
    "from scipy import sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Reading local cache file Outpatient.pkl\n"
     ]
    }
   ],
   "source": [
    "outpatient = ingest.get_cache_data(\"Outpatient\", \"Outpatient.pkl\")\n",
    "outpatient.dropna(subset=['CLM_PMT_AMT'], inplace=True)\n",
    "outpatient = outpatient[['CLM_PMT_AMT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIRTH_DATE\n",
      "(790790, 1)\n",
      "SEX\n",
      "(790790, 1)\n",
      "State\n",
      "(790790, 52)\n",
      "County\n",
      "(790790, 306)\n",
      "clm_dates\n",
      "(790790, 1)\n",
      "provider\n",
      "(790790, 284)\n",
      "provider_count\n",
      "(790790, 1)\n",
      "DGNS_CD\n",
      "(790790, 1001)\n",
      "DGNS_CD_count\n",
      "(790790, 1)\n",
      "PRDCR_CD\n",
      "(790790, 188)\n",
      "PRDCR_CD_count\n",
      "(790790, 1)\n",
      "HCPCS_CD\n",
      "(790790, 730)\n",
      "HCPCS_CD_count\n",
      "(790790, 1)\n"
     ]
    }
   ],
   "source": [
    "with open('feature_vectors_dictionary-truncated.txt', 'rb') as handle:\n",
    "    feature_dictionary = pickle.load(handle)\n",
    "\n",
    "for met_set in feature_dictionary.keys():\n",
    "    print(met_set)\n",
    "    feature_dictionary[met_set] = normalize(sparse.csr_matrix(feature_dictionary[met_set]))\n",
    "    print(np.shape(feature_dictionary[met_set]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(X, y, model, grid, feature_names):\n",
    "\n",
    "       ### Runs the model and returns MSE and R2 values and corresponding data as a dataframe\n",
    "\n",
    "       ### Takes in four arugments\n",
    "       ### X is the features to that model will be trained on as a sparse matrix\n",
    "       ### y is the scalar that is to be predicted as a list\n",
    "       ### model is the regression model as a model object\n",
    "       ### grid is a dictionary of model hyperparameters\n",
    "\n",
    "       ### returns a dataframe of results from the grid search\n",
    "\n",
    "       scoring = {'mean_squared_error': make_scorer(mean_squared_error), 'r2_score': make_scorer(r2_score)}\n",
    "       with parallel_backend('threading', n_jobs=2):\n",
    "              grid_model = GridSearchCV(model, grid, scoring=scoring, refit=False, return_train_score=True, error_score=\"raise\").fit(X.todense(), y)\n",
    "       \n",
    "       print(grid_model.cv_results_)\n",
    "\n",
    "       df=pd.DataFrame.from_dict(grid_model.cv_results_)\n",
    "       \n",
    "       return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_feature_sets(model, feature_dict, grid, feature_sets):\n",
    "\n",
    "    ### Creates and runs the specified model with each comboniation of features sets from 1 to k length features and stores the results of the models into a dictionary\n",
    "\n",
    "    ### Takes in four arguments\n",
    "    ### model is the regression model as a model object\n",
    "    ### feature_dict is the dictionary of feature sets\n",
    "    ### grid is a dictionary of model hyperparameters\n",
    "    ### feature_sets dicates how what feature sets will be returned, a list of list of strings corresponding to the keys in the dictionary\n",
    "    ### (continued) all would return all combinations for 1 through k, where  k is the number of keys in feature_dict\n",
    "\n",
    "    ### returns a dictionary of results\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for feature_list in feature_sets:\n",
    "        if feature_list == 'all':\n",
    "\n",
    "            for n_met in tqdm(range(len(feature_dict))):\n",
    "                for met_set in list(itertools.combinations(feature_dict.keys(), n_met+1)):\n",
    "                    for n_met_set in range(len(met_set)):\n",
    "                        if n_met_set == 0:\n",
    "                            X = feature_dict[met_set[n_met_set]]\n",
    "                        else:\n",
    "                            X  = sparse.hstack((X,feature_dict[met_set[n_met_set]]), format='csr')\n",
    "                    answer = run_model(X, outpatient['CLM_PMT_AMT'].values.tolist(), model, grid, met_set)\n",
    "                    results[met_set] = answer\n",
    "\n",
    "        else:\n",
    "            X = None\n",
    "            Name = ()\n",
    "            for i in feature_list:\n",
    "                if X == None:\n",
    "                    X = feature_dict[i]\n",
    "                    Name += (i,)\n",
    "                else:\n",
    "                    X = sparse.hstack((X, feature_dict[i]), format='csr')\n",
    "                    Name += (i,)\n",
    "            answer = run_model(X, outpatient['CLM_PMT_AMT'].values.tolist(), model, grid, feature_list)\n",
    "            results[Name] = answer\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results(results_dict, filename):\n",
    "\n",
    "    ### Takes in two arguements\n",
    "    ### results_dict is the dictionary of feature sets\n",
    "    ### file name is the wanted file name as a string\n",
    "\n",
    "    ### Returns None, but writes a file to the current directory\n",
    "\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for r_key in results_dict.keys():\n",
    "        temp_df = results_dict[r_key]\n",
    "        temp_df['features'] = str(r_key)\n",
    "        df = pd.concat((df, temp_df), ignore_index=True)\n",
    "\n",
    "    df.to_csv(filename)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Gradient Boosted Results\n",
    "### Purpose is to find which features sets matter before hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nGBRModel = HistGradientBoostingRegressor(random_state=42)\\nwrite_results(iterate_feature_sets(GBRModel, feature_dictionary, {}, [['clm_dates', 'provider', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD'],\\n['clm_dates', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD'],\\n['clm_dates', 'DGNS_CD', 'HCPCS_CD'],\\n['clm_dates', 'provider', 'DGNS_CD', 'HCPCS_CD'],\\n['DGNS_CD', 'PRDCR_CD', 'HCPCS_CD'],\\n['provider', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD'],\\n['provider', 'DGNS_CD', 'HCPCS_CD'],\\n['DGNS_CD', 'HCPCS_CD'],\\n['clm_dates', 'HCPCS_CD'],\\n['clm_dates', 'provider', 'PRDCR_CD', 'HCPCS_CD'],\\n['clm_dates', 'PRDCR_CD', 'HCPCS_CD'],\\n['clm_dates', 'provider', 'HCPCS_CD'],\\n['provider', 'PRDCR_CD', 'HCPCS_CD'],\\n['provider', 'HCPCS_CD'],\\n['HCPCS_CD'],\\n['PRDCR_CD', 'HCPCS_CD'],\\n['clm_dates', 'DGNS_CD', 'PRDCR_CD'],\\n['clm_dates', 'provider', 'DGNS_CD'],\\n['clm_dates', 'DGNS_CD'],\\n['clm_dates', 'provider', 'DGNS_CD', 'PRDCR_CD'],\\n['DGNS_CD', 'PRDCR_CD'],\\n['provider', 'DGNS_CD'],\\n['provider', 'DGNS_CD', 'PRDCR_CD'],\\n['DGNS_CD'],\\n['clm_dates'],\\n['clm_dates', 'PRDCR_CD'],\\n['clm_dates', 'provider'],\\n['clm_dates', 'provider', 'PRDCR_CD'],\\n['provider', 'PRDCR_CD'],\\n['PRDCR_CD'],\\n['provider']]), 'Initial_GBRModel.csv')\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "GBRModel = HistGradientBoostingRegressor(random_state=42)\n",
    "write_results(iterate_feature_sets(GBRModel, feature_dictionary, {}, [['clm_dates', 'provider', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD'],\n",
    "['clm_dates', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD'],\n",
    "['clm_dates', 'DGNS_CD', 'HCPCS_CD'],\n",
    "['clm_dates', 'provider', 'DGNS_CD', 'HCPCS_CD'],\n",
    "['DGNS_CD', 'PRDCR_CD', 'HCPCS_CD'],\n",
    "['provider', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD'],\n",
    "['provider', 'DGNS_CD', 'HCPCS_CD'],\n",
    "['DGNS_CD', 'HCPCS_CD'],\n",
    "['clm_dates', 'HCPCS_CD'],\n",
    "['clm_dates', 'provider', 'PRDCR_CD', 'HCPCS_CD'],\n",
    "['clm_dates', 'PRDCR_CD', 'HCPCS_CD'],\n",
    "['clm_dates', 'provider', 'HCPCS_CD'],\n",
    "['provider', 'PRDCR_CD', 'HCPCS_CD'],\n",
    "['provider', 'HCPCS_CD'],\n",
    "['HCPCS_CD'],\n",
    "['PRDCR_CD', 'HCPCS_CD'],\n",
    "['clm_dates', 'DGNS_CD', 'PRDCR_CD'],\n",
    "['clm_dates', 'provider', 'DGNS_CD'],\n",
    "['clm_dates', 'DGNS_CD'],\n",
    "['clm_dates', 'provider', 'DGNS_CD', 'PRDCR_CD'],\n",
    "['DGNS_CD', 'PRDCR_CD'],\n",
    "['provider', 'DGNS_CD'],\n",
    "['provider', 'DGNS_CD', 'PRDCR_CD'],\n",
    "['DGNS_CD'],\n",
    "['clm_dates'],\n",
    "['clm_dates', 'PRDCR_CD'],\n",
    "['clm_dates', 'provider'],\n",
    "['clm_dates', 'provider', 'PRDCR_CD'],\n",
    "['provider', 'PRDCR_CD'],\n",
    "['PRDCR_CD'],\n",
    "['provider']]), 'Initial_GBRModel.csv')\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Stochastic Gradient Descent Results\n",
    "### Purpose is to find which features set's matter before hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSGDModel = SGDRegressor(random_state=42)\\nwrite_results(iterate_feature_sets(SGDModel, feature_dictionary, {}, [['clm_dates', 'provider', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD'],\\n['clm_dates', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD'],\\n['clm_dates', 'DGNS_CD', 'HCPCS_CD'],\\n['clm_dates', 'provider', 'DGNS_CD', 'HCPCS_CD'],\\n['DGNS_CD', 'PRDCR_CD', 'HCPCS_CD'],\\n['provider', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD'],\\n['provider', 'DGNS_CD', 'HCPCS_CD'],\\n['DGNS_CD', 'HCPCS_CD'],\\n['clm_dates', 'HCPCS_CD'],\\n['clm_dates', 'provider', 'PRDCR_CD', 'HCPCS_CD'],\\n['clm_dates', 'PRDCR_CD', 'HCPCS_CD'],\\n['clm_dates', 'provider', 'HCPCS_CD'],\\n['provider', 'PRDCR_CD', 'HCPCS_CD'],\\n['provider', 'HCPCS_CD'],\\n['HCPCS_CD'],\\n['PRDCR_CD', 'HCPCS_CD'],\\n['clm_dates', 'DGNS_CD', 'PRDCR_CD'],\\n['clm_dates', 'provider', 'DGNS_CD'],\\n['clm_dates', 'DGNS_CD'],\\n['clm_dates', 'provider', 'DGNS_CD', 'PRDCR_CD'],\\n['DGNS_CD', 'PRDCR_CD'],\\n['provider', 'DGNS_CD'],\\n['provider', 'DGNS_CD', 'PRDCR_CD'],\\n['DGNS_CD'],\\n['clm_dates'],\\n['clm_dates', 'PRDCR_CD'],\\n['clm_dates', 'provider'],\\n['clm_dates', 'provider', 'PRDCR_CD'],\\n['provider', 'PRDCR_CD'],\\n['PRDCR_CD'],\\n['provider']]), 'Initial_SGDModel.csv')\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "SGDModel = SGDRegressor(random_state=42)\n",
    "write_results(iterate_feature_sets(SGDModel, feature_dictionary, {}, [['clm_dates', 'provider', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD'],\n",
    "['clm_dates', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD'],\n",
    "['clm_dates', 'DGNS_CD', 'HCPCS_CD'],\n",
    "['clm_dates', 'provider', 'DGNS_CD', 'HCPCS_CD'],\n",
    "['DGNS_CD', 'PRDCR_CD', 'HCPCS_CD'],\n",
    "['provider', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD'],\n",
    "['provider', 'DGNS_CD', 'HCPCS_CD'],\n",
    "['DGNS_CD', 'HCPCS_CD'],\n",
    "['clm_dates', 'HCPCS_CD'],\n",
    "['clm_dates', 'provider', 'PRDCR_CD', 'HCPCS_CD'],\n",
    "['clm_dates', 'PRDCR_CD', 'HCPCS_CD'],\n",
    "['clm_dates', 'provider', 'HCPCS_CD'],\n",
    "['provider', 'PRDCR_CD', 'HCPCS_CD'],\n",
    "['provider', 'HCPCS_CD'],\n",
    "['HCPCS_CD'],\n",
    "['PRDCR_CD', 'HCPCS_CD'],\n",
    "['clm_dates', 'DGNS_CD', 'PRDCR_CD'],\n",
    "['clm_dates', 'provider', 'DGNS_CD'],\n",
    "['clm_dates', 'DGNS_CD'],\n",
    "['clm_dates', 'provider', 'DGNS_CD', 'PRDCR_CD'],\n",
    "['DGNS_CD', 'PRDCR_CD'],\n",
    "['provider', 'DGNS_CD'],\n",
    "['provider', 'DGNS_CD', 'PRDCR_CD'],\n",
    "['DGNS_CD'],\n",
    "['clm_dates'],\n",
    "['clm_dates', 'PRDCR_CD'],\n",
    "['clm_dates', 'provider'],\n",
    "['clm_dates', 'provider', 'PRDCR_CD'],\n",
    "['provider', 'PRDCR_CD'],\n",
    "['PRDCR_CD'],\n",
    "['provider']]), 'Initial_SGDModel.csv')\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Elastic-Net Results\n",
    "### Purpose is to find which features set's matter before hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nENModel = ElasticNet(random_state=42)\\nwrite_results(iterate_feature_sets(ENModel, feature_dictionary, {}, [['clm_dates', 'provider', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD'],\\n['clm_dates', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD'],\\n['clm_dates', 'DGNS_CD', 'HCPCS_CD'],\\n['clm_dates', 'provider', 'DGNS_CD', 'HCPCS_CD'],\\n['DGNS_CD', 'PRDCR_CD', 'HCPCS_CD'],\\n['provider', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD'],\\n['provider', 'DGNS_CD', 'HCPCS_CD'],\\n['DGNS_CD', 'HCPCS_CD'],\\n['clm_dates', 'HCPCS_CD'],\\n['clm_dates', 'provider', 'PRDCR_CD', 'HCPCS_CD'],\\n['clm_dates', 'PRDCR_CD', 'HCPCS_CD'],\\n['clm_dates', 'provider', 'HCPCS_CD'],\\n['provider', 'PRDCR_CD', 'HCPCS_CD'],\\n['provider', 'HCPCS_CD'],\\n['HCPCS_CD'],\\n['PRDCR_CD', 'HCPCS_CD'],\\n['clm_dates', 'DGNS_CD', 'PRDCR_CD'],\\n['clm_dates', 'provider', 'DGNS_CD'],\\n['clm_dates', 'DGNS_CD'],\\n['clm_dates', 'provider', 'DGNS_CD', 'PRDCR_CD'],\\n['DGNS_CD', 'PRDCR_CD'],\\n['provider', 'DGNS_CD'],\\n['provider', 'DGNS_CD', 'PRDCR_CD'],\\n['DGNS_CD'],\\n['clm_dates'],\\n['clm_dates', 'PRDCR_CD'],\\n['clm_dates', 'provider'],\\n['clm_dates', 'provider', 'PRDCR_CD'],\\n['provider', 'PRDCR_CD'],\\n['PRDCR_CD'],\\n['provider']]), 'Initial_ENModel.csv')\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ENModel = ElasticNet(random_state=42)\n",
    "write_results(iterate_feature_sets(ENModel, feature_dictionary, {}, [['clm_dates', 'provider', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD'],\n",
    "['clm_dates', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD'],\n",
    "['clm_dates', 'DGNS_CD', 'HCPCS_CD'],\n",
    "['clm_dates', 'provider', 'DGNS_CD', 'HCPCS_CD'],\n",
    "['DGNS_CD', 'PRDCR_CD', 'HCPCS_CD'],\n",
    "['provider', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD'],\n",
    "['provider', 'DGNS_CD', 'HCPCS_CD'],\n",
    "['DGNS_CD', 'HCPCS_CD'],\n",
    "['clm_dates', 'HCPCS_CD'],\n",
    "['clm_dates', 'provider', 'PRDCR_CD', 'HCPCS_CD'],\n",
    "['clm_dates', 'PRDCR_CD', 'HCPCS_CD'],\n",
    "['clm_dates', 'provider', 'HCPCS_CD'],\n",
    "['provider', 'PRDCR_CD', 'HCPCS_CD'],\n",
    "['provider', 'HCPCS_CD'],\n",
    "['HCPCS_CD'],\n",
    "['PRDCR_CD', 'HCPCS_CD'],\n",
    "['clm_dates', 'DGNS_CD', 'PRDCR_CD'],\n",
    "['clm_dates', 'provider', 'DGNS_CD'],\n",
    "['clm_dates', 'DGNS_CD'],\n",
    "['clm_dates', 'provider', 'DGNS_CD', 'PRDCR_CD'],\n",
    "['DGNS_CD', 'PRDCR_CD'],\n",
    "['provider', 'DGNS_CD'],\n",
    "['provider', 'DGNS_CD', 'PRDCR_CD'],\n",
    "['DGNS_CD'],\n",
    "['clm_dates'],\n",
    "['clm_dates', 'PRDCR_CD'],\n",
    "['clm_dates', 'provider'],\n",
    "['clm_dates', 'provider', 'PRDCR_CD'],\n",
    "['provider', 'PRDCR_CD'],\n",
    "['PRDCR_CD'],\n",
    "['provider']]), 'Initial_ENModel.csv')\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial XGBoost Results\n",
    "### Purpose is to find which features set's matter before hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nXGBModel = xgb.(random_state=42)\\nwrite_results(iterate_feature_sets(XGBModel, feature_dictionary, {}, [['clm_dates', 'provider', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD'],\\n['clm_dates', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD'],\\n['clm_dates', 'DGNS_CD', 'HCPCS_CD'],\\n['clm_dates', 'provider', 'DGNS_CD', 'HCPCS_CD'],\\n['DGNS_CD', 'PRDCR_CD', 'HCPCS_CD'],\\n['provider', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD'],\\n['provider', 'DGNS_CD', 'HCPCS_CD'],\\n['DGNS_CD', 'HCPCS_CD'],\\n['clm_dates', 'HCPCS_CD'],\\n['clm_dates', 'provider', 'PRDCR_CD', 'HCPCS_CD'],\\n['clm_dates', 'PRDCR_CD', 'HCPCS_CD'],\\n['clm_dates', 'provider', 'HCPCS_CD'],\\n['provider', 'PRDCR_CD', 'HCPCS_CD'],\\n['provider', 'HCPCS_CD'],\\n['HCPCS_CD'],\\n['PRDCR_CD', 'HCPCS_CD'],\\n['clm_dates', 'DGNS_CD', 'PRDCR_CD'],\\n['clm_dates', 'provider', 'DGNS_CD'],\\n['clm_dates', 'DGNS_CD'],\\n['clm_dates', 'provider', 'DGNS_CD', 'PRDCR_CD'],\\n['DGNS_CD', 'PRDCR_CD'],\\n['provider', 'DGNS_CD'],\\n['provider', 'DGNS_CD', 'PRDCR_CD'],\\n['DGNS_CD'],\\n['clm_dates'],\\n['clm_dates', 'PRDCR_CD'],\\n['clm_dates', 'provider'],\\n['clm_dates', 'provider', 'PRDCR_CD'],\\n['provider', 'PRDCR_CD'],\\n['PRDCR_CD'],\\n['provider']]), 'InitiaXGBModel.csv')\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "XGBModel = xgb.(random_state=42)\n",
    "write_results(iterate_feature_sets(XGBModel, feature_dictionary, {}, [['clm_dates', 'provider', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD'],\n",
    "['clm_dates', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD'],\n",
    "['clm_dates', 'DGNS_CD', 'HCPCS_CD'],\n",
    "['clm_dates', 'provider', 'DGNS_CD', 'HCPCS_CD'],\n",
    "['DGNS_CD', 'PRDCR_CD', 'HCPCS_CD'],\n",
    "['provider', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD'],\n",
    "['provider', 'DGNS_CD', 'HCPCS_CD'],\n",
    "['DGNS_CD', 'HCPCS_CD'],\n",
    "['clm_dates', 'HCPCS_CD'],\n",
    "['clm_dates', 'provider', 'PRDCR_CD', 'HCPCS_CD'],\n",
    "['clm_dates', 'PRDCR_CD', 'HCPCS_CD'],\n",
    "['clm_dates', 'provider', 'HCPCS_CD'],\n",
    "['provider', 'PRDCR_CD', 'HCPCS_CD'],\n",
    "['provider', 'HCPCS_CD'],\n",
    "['HCPCS_CD'],\n",
    "['PRDCR_CD', 'HCPCS_CD'],\n",
    "['clm_dates', 'DGNS_CD', 'PRDCR_CD'],\n",
    "['clm_dates', 'provider', 'DGNS_CD'],\n",
    "['clm_dates', 'DGNS_CD'],\n",
    "['clm_dates', 'provider', 'DGNS_CD', 'PRDCR_CD'],\n",
    "['DGNS_CD', 'PRDCR_CD'],\n",
    "['provider', 'DGNS_CD'],\n",
    "['provider', 'DGNS_CD', 'PRDCR_CD'],\n",
    "['DGNS_CD'],\n",
    "['clm_dates'],\n",
    "['clm_dates', 'PRDCR_CD'],\n",
    "['clm_dates', 'provider'],\n",
    "['clm_dates', 'provider', 'PRDCR_CD'],\n",
    "['provider', 'PRDCR_CD'],\n",
    "['PRDCR_CD'],\n",
    "['provider']]), 'InitiaXGBModel.csv')\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search Gradient Boosted Results\n",
    "### Purpose is to find which hyperparameters perform the best on the top three feature selections + our demographic information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ngrid_dict = {#'learning_rate': [1, 0.5, 0.25, 0.1, 0.05, 0.01],\\n    #'max_iter': [1, 2, 4, 8, 16, 32, 64, 100, 200],\\n    'max_leaf_nodes': [2, 4, 6],\\n    #'max_depth': [np.linspace(1, 32, 32, endpoint=True)],\\n    #'min_samples_leaf': [np.linspace(0.1, 0.5, 5, endpoint=True)]\\n    }\\n\\n\\nGBRModel = HistGradientBoostingRegressor(random_state=42)\\nfor search in grid_dict.keys():\\n    write_results(iterate_feature_sets(GBRModel, feature_dictionary, {search: grid_dict[search]}, [\\n        ['clm_dates', 'provider', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD', 'BIRTH_DATE', 'SEX', 'State', 'County'],\\n        ['clm_dates', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD', 'BIRTH_DATE', 'SEX', 'State', 'County'],\\n        ['clm_dates', 'DGNS_CD', 'HCPCS_CD', 'BIRTH_DATE', 'SEX', 'State', 'County']]), search+'_GridSearch_GBRModel2.csv')\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "grid_dict = {#'learning_rate': [1, 0.5, 0.25, 0.1, 0.05, 0.01],\n",
    "    #'max_iter': [1, 2, 4, 8, 16, 32, 64, 100, 200],\n",
    "    'max_leaf_nodes': [2, 4, 6],\n",
    "    #'max_depth': [np.linspace(1, 32, 32, endpoint=True)],\n",
    "    #'min_samples_leaf': [np.linspace(0.1, 0.5, 5, endpoint=True)]\n",
    "    }\n",
    "\n",
    "\n",
    "GBRModel = HistGradientBoostingRegressor(random_state=42)\n",
    "for search in grid_dict.keys():\n",
    "    write_results(iterate_feature_sets(GBRModel, feature_dictionary, {search: grid_dict[search]}, [\n",
    "        ['clm_dates', 'provider', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD', 'BIRTH_DATE', 'SEX', 'State', 'County'],\n",
    "        ['clm_dates', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD', 'BIRTH_DATE', 'SEX', 'State', 'County'],\n",
    "        ['clm_dates', 'DGNS_CD', 'HCPCS_CD', 'BIRTH_DATE', 'SEX', 'State', 'County']]), search+'_GridSearch_GBRModel2.csv')\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search Stochastic Gradient Descent Results\n",
    "### Purpose is to find which hyperparameters perform the best on the top three feature selections + our demographic information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ngrid_dict = {'loss': ['squared_loss', 'squared_epsilon_insensitive'],\\n    'penalty': ['l1', 'l2'],\\n    'alpha': [.001, .0001, .00001],\\n    'max_iter': [1000, 5000, 10000]}\\n\\nSGDModel = SGDRegressor(random_state=42)\\nfor search in grid_dict.keys():\\n    write_results(iterate_feature_sets(SGDModel, feature_dictionary, {search: grid_dict[search]}, [\\n['clm_dates', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD', 'BIRTH_DATE', 'SEX', 'State', 'County'],\\n['clm_dates', 'DGNS_CD', 'HCPCS_CD', 'BIRTH_DATE', 'SEX', 'State', 'County'],\\n['clm_dates', 'provider', 'DGNS_CD', 'HCPCS_CD', 'BIRTH_DATE', 'SEX', 'State', 'County']]), search+'_GridSearch_SGDModel.csv')\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "grid_dict = {'loss': ['squared_loss', 'squared_epsilon_insensitive'],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'alpha': [.001, .0001, .00001],\n",
    "    'max_iter': [1000, 5000, 10000]}\n",
    "\n",
    "SGDModel = SGDRegressor(random_state=42)\n",
    "for search in grid_dict.keys():\n",
    "    write_results(iterate_feature_sets(SGDModel, feature_dictionary, {search: grid_dict[search]}, [\n",
    "['clm_dates', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD', 'BIRTH_DATE', 'SEX', 'State', 'County'],\n",
    "['clm_dates', 'DGNS_CD', 'HCPCS_CD', 'BIRTH_DATE', 'SEX', 'State', 'County'],\n",
    "['clm_dates', 'provider', 'DGNS_CD', 'HCPCS_CD', 'BIRTH_DATE', 'SEX', 'State', 'County']]), search+'_GridSearch_SGDModel.csv')\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search Elastic Net Results\n",
    "### Purpose is to find which hyperparameters perform the best on the top three feature selections + our demographic information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ngrid_dict = {'alpha': [.0001, .001, .01],\\n            'l1_ratio': [.1, .2, .3, .4, .5, .6, .7, .8, .9]}\\n\\nENModel = ElasticNet(random_state=42)\\nfor search in grid_dict.keys():\\n    write_results(iterate_feature_sets(ENModel, feature_dictionary, {search: grid_dict[search]}, [\\n['clm_dates', 'DGNS_CD', 'HCPCS_CD', 'BIRTH_DATE', 'SEX', 'State', 'County'],\\n['clm_dates', 'DGNS_CD', 'HCPCS_CD', 'PRDCR_CD', 'BIRTH_DATE', 'SEX', 'State', 'County'],\\n['clm_dates', 'provider', 'DGNS_CD', 'HCPCS_CD', 'BIRTH_DATE', 'SEX', 'State', 'County']]), search+'_GridSearch_ENModel.csv')\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "grid_dict = {'alpha': [.0001, .001, .01],\n",
    "            'l1_ratio': [.1, .2, .3, .4, .5, .6, .7, .8, .9]}\n",
    "\n",
    "ENModel = ElasticNet(random_state=42)\n",
    "for search in grid_dict.keys():\n",
    "    write_results(iterate_feature_sets(ENModel, feature_dictionary, {search: grid_dict[search]}, [\n",
    "['clm_dates', 'DGNS_CD', 'HCPCS_CD', 'BIRTH_DATE', 'SEX', 'State', 'County'],\n",
    "['clm_dates', 'DGNS_CD', 'HCPCS_CD', 'PRDCR_CD', 'BIRTH_DATE', 'SEX', 'State', 'County'],\n",
    "['clm_dates', 'provider', 'DGNS_CD', 'HCPCS_CD', 'BIRTH_DATE', 'SEX', 'State', 'County']]), search+'_GridSearch_ENModel.csv')\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search XGBoost Results\n",
    "### Purpose is to find which hyperparameters perform the best on the top three feature selections + our demographic information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ngrid_dict = {'n_estimators': [3, 6, 9, 12, 15, 18],\\n    'tree_method': ['approx', 'hist'],\\n    'max_depth': [5,10,15,20,25,30,35]}\\n\\n\\nXGBModel = xgb.XGBRFRegressor(random_state=42)\\nfor search in grid_dict.keys():\\n    write_results(iterate_feature_sets(XGBModel, feature_dictionary, {search: grid_dict[search]}, [\\n        ['clm_dates', 'provider', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD', 'BIRTH_DATE', 'SEX', 'State', 'County'],\\n        ['clm_dates', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD', 'BIRTH_DATE', 'SEX', 'State', 'County'],\\n        ['clm_dates', 'DGNS_CD', 'HCPCS_CD', 'BIRTH_DATE', 'SEX', 'State', 'County']]), search+'_GridSearch_XGBModel.csv')\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "grid_dict = {'n_estimators': [3, 6, 9, 12, 15, 18],\n",
    "    'tree_method': ['approx', 'hist'],\n",
    "    'max_depth': [5,10,15,20,25,30,35]}\n",
    "\n",
    "\n",
    "XGBModel = xgb.XGBRFRegressor(random_state=42)\n",
    "for search in grid_dict.keys():\n",
    "    write_results(iterate_feature_sets(XGBModel, feature_dictionary, {search: grid_dict[search]}, [\n",
    "        ['clm_dates', 'provider', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD', 'BIRTH_DATE', 'SEX', 'State', 'County'],\n",
    "        ['clm_dates', 'DGNS_CD', 'PRDCR_CD', 'HCPCS_CD', 'BIRTH_DATE', 'SEX', 'State', 'County'],\n",
    "        ['clm_dates', 'DGNS_CD', 'HCPCS_CD', 'BIRTH_DATE', 'SEX', 'State', 'County']]), search+'_GridSearch_XGBModel.csv')\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Histogram Gradient Boosted NonPCA Supervised Run\n",
    "### See how the model performs on the dataset without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grid_dict = {'learning_rate': [0.25],\n",
    "    'max_iter': [200],\n",
    "    #wanted to go a little higher on leaf nodes since there seemed to be more to gain\n",
    "    'max_leaf_nodes': [10],\n",
    "    'max_depth': [18]\n",
    "    #results were inconclusive for minimum samples, going to leave as default\n",
    "    }\n",
    "\n",
    "\n",
    "GBRModel = HistGradientBoostingRegressor(random_state=42)\n",
    "write_results(iterate_feature_sets(GBRModel, feature_dictionary, grid_dict, [\n",
    "    # picked because of speed and results\n",
    "    ['clm_dates', 'DGNS_CD', 'HCPCS_CD', 'BIRTH_DATE', 'SEX', 'State', 'County']]), 'NPCA_Final_GBRModel.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Stochastic Gradient Descent NonPCA Supervised Run\n",
    "### See how the model performs on the dataset without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SGDRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\TAYLOR~1\\AppData\\Local\\Temp/ipykernel_1296/3672544880.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     'max_iter': [1000]}\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mSGDModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSGDRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m write_results(iterate_feature_sets(SGDModel, feature_dictionary, grid_dict, [\n\u001b[0;32m      8\u001b[0m ['clm_dates', 'DGNS_CD', 'HCPCS_CD', 'BIRTH_DATE', 'SEX', 'State', 'County']]), 'NPCA_Final_SGDModel.csv')\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SGDRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "grid_dict = {'loss': ['squared_epsilon_insensitive'],\n",
    "    'penalty': ['l1'],\n",
    "    'alpha': [.0001],\n",
    "    'max_iter': [1000]}\n",
    "\n",
    "SGDModel = SGDRegressor(random_state=42)\n",
    "write_results(iterate_feature_sets(SGDModel, feature_dictionary, grid_dict, [\n",
    "['clm_dates', 'DGNS_CD', 'HCPCS_CD', 'BIRTH_DATE', 'SEX', 'State', 'County']]), 'NPCA_Final_SGDModel.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Elastic Net NonPCA Supervised Run\n",
    "### See how the model performs on the dataset without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([1313.70082536]), 'std_fit_time': array([173.4870673]), 'mean_score_time': array([3.42845478]), 'std_score_time': array([0.60479199]), 'param_alpha': masked_array(data=[0.0001],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_l1_ratio': masked_array(data=[0.9],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'alpha': 0.0001, 'l1_ratio': 0.9}], 'split0_test_mean_squared_error': array([234575.36121982]), 'split1_test_mean_squared_error': array([236388.11086495]), 'split2_test_mean_squared_error': array([237540.75215016]), 'split3_test_mean_squared_error': array([243170.68852935]), 'split4_test_mean_squared_error': array([242236.20190961]), 'mean_test_mean_squared_error': array([238782.22293478]), 'std_test_mean_squared_error': array([3351.39698735]), 'rank_test_mean_squared_error': array([1]), 'split0_train_mean_squared_error': array([238319.35366125]), 'split1_train_mean_squared_error': array([237876.28868492]), 'split2_train_mean_squared_error': array([237631.44180044]), 'split3_train_mean_squared_error': array([236188.56603566]), 'split4_train_mean_squared_error': array([236403.98491964]), 'mean_train_mean_squared_error': array([237283.92702038]), 'std_train_mean_squared_error': array([838.79448974]), 'split0_test_r2_score': array([0.26241763]), 'split1_test_r2_score': array([0.25777969]), 'split2_test_r2_score': array([0.26337533]), 'split3_test_r2_score': array([0.27451431]), 'split4_test_r2_score': array([0.28367539]), 'mean_test_r2_score': array([0.26835247]), 'std_test_r2_score': array([0.00942909]), 'rank_test_r2_score': array([1]), 'split0_train_r2_score': array([0.27474073]), 'split1_train_r2_score': array([0.27583404]), 'split2_train_r2_score': array([0.27438957]), 'split3_train_r2_score': array([0.27172154]), 'split4_train_r2_score': array([0.26935561]), 'mean_train_r2_score': array([0.2732083]), 'std_train_r2_score': array([0.00235338])}\n"
     ]
    }
   ],
   "source": [
    "grid_dict = {'alpha': [.0001],\n",
    "    'l1_ratio': [.9]}\n",
    "\n",
    "ENModel = ElasticNet(random_state=42)\n",
    "write_results(iterate_feature_sets(ENModel, feature_dictionary, grid_dict, [\n",
    "['clm_dates', 'DGNS_CD', 'HCPCS_CD', 'BIRTH_DATE', 'SEX', 'State', 'County']]), 'NPCA_Final_ENModel.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final XGBoost NonPCA Supervised Run\n",
    "### See how the model performs on the dataset without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([255.17862582]), 'std_fit_time': array([41.15142138]), 'mean_score_time': array([2.32038045]), 'std_score_time': array([0.69958266]), 'param_max_depth': masked_array(data=[10],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_number_of_estimators': masked_array(data=[9],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_tree_method': masked_array(data=['hist'],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'max_depth': 10, 'number_of_estimators': 9, 'tree_method': 'hist'}], 'split0_test_mean_squared_error': array([229596.14047328]), 'split1_test_mean_squared_error': array([229405.51322255]), 'split2_test_mean_squared_error': array([231170.87972658]), 'split3_test_mean_squared_error': array([235786.80856708]), 'split4_test_mean_squared_error': array([234357.414672]), 'mean_test_mean_squared_error': array([232063.3513323]), 'std_test_mean_squared_error': array([2571.94863064]), 'rank_test_mean_squared_error': array([1]), 'split0_train_mean_squared_error': array([228615.28811887]), 'split1_train_mean_squared_error': array([228581.99781181]), 'split2_train_mean_squared_error': array([228276.86057634]), 'split3_train_mean_squared_error': array([227115.47023421]), 'split4_train_mean_squared_error': array([227350.83929695]), 'mean_train_mean_squared_error': array([227988.09120764]), 'std_train_mean_squared_error': array([631.98831147]), 'split0_test_r2_score': array([0.27807395]), 'split1_test_r2_score': array([0.27970391]), 'split2_test_r2_score': array([0.28312859]), 'split3_test_r2_score': array([0.29654369]), 'split4_test_r2_score': array([0.30697401]), 'mean_test_r2_score': array([0.28888483]), 'std_test_r2_score': array([0.0111371]), 'rank_test_r2_score': array([1]), 'split0_train_r2_score': array([0.30427238]), 'split1_train_r2_score': array([0.30412862]), 'split2_train_r2_score': array([0.3029539]), 'split3_train_r2_score': array([0.29969809]), 'split4_train_r2_score': array([0.2973358]), 'mean_train_r2_score': array([0.30167776]), 'std_train_r2_score': array([0.0027254])}\n"
     ]
    }
   ],
   "source": [
    "grid_dict = {'max_depth': [10],\n",
    "    'number_of_estimators': [9],\n",
    "    'tree_method': ['hist']}\n",
    "\n",
    "XGBModel = xgb.XGBRFRegressor(random_state=42)\n",
    "write_results(iterate_feature_sets(XGBModel, feature_dictionary, grid_dict, [\n",
    "['clm_dates', 'DGNS_CD', 'HCPCS_CD', 'BIRTH_DATE', 'SEX', 'State', 'County']]), 'NPCA_Final_XGBModel.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
